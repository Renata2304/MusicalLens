Layer Shape Note
Input w × 64 × 1
Convolution w × 64 × 16 Kernel 5x5
Max pooling w/2 × 32 × 16 Stride 2,2
Convolution w/2 × 32 × 32 Kernel 5x5
Max pooling w/4 × 16 × 32 Stride 2,2
Convolution w/4 × 16 × 64 Kernel 5x5
Max pooling w/4 × 8 × 64 Stride 1,2
Convolution w/4 × 8 × 128 Kernel 3x3
Max pooling w/4 × 4 × 128 Stride 1,2
Convolution w/4 × 4 × 128 Kernel 3x3
Max pooling w/4 × 2 × 128 Stride 1,2
Convolution w/4 × 2 × 256 Kernel 3x3
Max pooling w/4 × 1 × 256 Stride 1,2
Reshape w/4 × 256
BLSTM w/4 × 256 + w/4 × 256 Droupout
Concatenate w/4 × 512
Fully connected w/4 × num_classes + 1 No activation function
CTC ≤ w/4


import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

def create_omr_model(input_shape, num_classes):
    # Input Layer
    inputs = keras.Input(shape=input_shape, name="image") # input_shape va fi (None, 64, 1) pentru w x 64 x 1

    # --- CNN Backbone ---
    # Convolution 1
    x = layers.Conv2D(16, (5, 5), activation="relu", padding="same")(inputs)
    # Max Pooling 1
    x = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding="valid")(x)

    # Convolution 2
    x = layers.Conv2D(32, (5, 5), activation="relu", padding="same")(x)
    # Max Pooling 2
    x = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding="valid")(x)

    # Convolution 3
    x = layers.Conv2D(64, (5, 5), activation="relu", padding="same")(x)
    # Max Pooling 3 (Stride 1,2)
    x = layers.MaxPooling2D(pool_size=(1, 2), strides=(1, 2), padding="valid")(x)

    # Convolution 4
    x = layers.Conv2D(128, (3, 3), activation="relu", padding="same")(x)
    # Max Pooling 4 (Stride 1,2)
    x = layers.MaxPooling2D(pool_size=(1, 2), strides=(1, 2), padding="valid")(x)

    # Convolution 5
    x = layers.Conv2D(128, (3, 3), activation="relu", padding="same")(x)
    # Max Pooling 5 (Stride 1,2)
    x = layers.MaxPooling2D(pool_size=(1, 2), strides=(1, 2), padding="valid")(x)

    # Convolution 6
    x = layers.Conv2D(256, (3, 3), activation="relu", padding="same")(x)
    # Max Pooling 6 (Stride 1,2)
    x = layers.MaxPooling2D(pool_size=(1, 2), strides=(1, 2), padding="valid")(x)

    # --- Reshape for RNN ---
    # `w/4` din descriere devine `width_after_cnn`
    # `height_after_cnn` ar trebui să fie 1 după ultimul pooling
    # `channels_after_cnn` ar trebui să fie 256
    
    # Calculate output dimensions after CNN to reshape correctly
    # Keras reshape expects (batch_size, num_steps, features)
    # We need to flatten the height (which is 1) and combine with channels
    shape = x.get_shape()
    # Assuming shape[1] is width (w/4), shape[2] is height (1), shape[3] is channels (256)
    # We want to reshape to (width_after_cnn, channels_after_cnn * height_after_cnn)
    reshaped = layers.Reshape((shape[1], shape[2] * shape[3]))(x) # (w/4, 1 * 256) -> (w/4, 256)

    # --- RNN (BLSTM) ---
    # Bidirectional LSTM layer
    blstm = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(reshaped) # 256 units * 2 directions = 512 total
    blstm = layers.Dropout(0.2)(blstm) # Exemplu de dropout rate, ajustează după nevoie

    # --- Output Layer ---
    # Fully Connected layer for classification
    # Output units: num_classes + 1 (for CTC blank token)
    outputs = layers.Dense(num_classes + 1, activation=None)(blstm) # No activation for logits for CTC

    # Create the model
    model = keras.Model(inputs=inputs, outputs=outputs)
    return model

# Example Usage:
# Assuming num_classes is the number of unique musical symbols + 1 for blank
# For instance, if you have 80 unique symbols: num_classes = 80
num_classes_omr = 80 # Aici vei pune numărul real de clase + 1 pentru CTC blank
input_img_shape = (None, 64, 1) # None pentru lățime variabilă, 64 înălțime, 1 canal

omr_model = create_omr_model(input_img_shape, num_classes_omr)
omr_model.summary()

# --- Compilare și antrenare ---
# Pentru antrenare, vei avea nevoie de o funcție de pierdere CTC (TensorFlow are keras_ocr.tools.CTC)
# și de date de antrenament: imagini și secvențe de etichete.
# Antrenarea cu CTC este mai complexă și implică un `custom training loop` sau folosirea
# de `tf.keras.Model.compile` cu `ctc_batch_cost` ca funcție de pierdere.